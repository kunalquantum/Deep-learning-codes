{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dbae852c",
   "metadata": {},
   "source": [
    "# #Buildiing the First Neural Network ......"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c5cc5411",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21973bad",
   "metadata": {},
   "source": [
    "import torch: This imports the main PyTorch library, which provides a wide range of functionalities for building and training neural networks.\n",
    "\n",
    "import torch.nn as nn: This imports the submodule nn from PyTorch, which includes classes and functions for defining and working with neural networks.\n",
    "\n",
    "import torch.optim as optim: This imports the submodule optim from PyTorch, which contains various optimization algorithms for updating the parameters of neural networks during training.\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset: This imports the classes DataLoader and Dataset from the torch.utils.data module. These classes are used for efficiently loading and handling datasets during the training process.\n",
    "\n",
    "from sklearn.datasets import load_iris: This imports the function load_iris from scikit-learn, a popular machine learning library. load_iris is used to load the Iris dataset, a commonly used dataset for classification tasks.\n",
    "\n",
    "from sklearn.model_selection import train_test_split: This imports the function train_test_split from scikit-learn. It is used to split the dataset into training and testing subsets.\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler: This imports the class StandardScaler from scikit-learn. StandardScaler is used for standardizing the features of the dataset, ensuring that they have zero mean and unit variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf782d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1845239",
   "metadata": {},
   "source": [
    "load_iris() from scikit-learn is used to load the Iris dataset, which is a classic machine learning dataset containing\n",
    "\n",
    "measurements of flower samples from three different species of Iris flowers.\n",
    "\n",
    "iris.data represents the input features of the dataset, which includes measurements such as sepal length, sepal width, petal length, and petal width.\n",
    "\n",
    "iris.target represents the corresponding target labels, which indicate the species of each sample.\n",
    "\n",
    "train_test_split() from scikit-learn is used to split the data into training and test sets. It takes the input features (X)\n",
    "\n",
    "and the corresponding target labels (y) as input.\n",
    "\n",
    "The test_size parameter is set to 0.2, indicating that 20% of the data will be allocated for testing, while the remaining\n",
    "\n",
    "80% will be used for training.\n",
    "\n",
    "The random_state parameter is set to 42, which ensures reproducibility by fixing the random seed. This means that the same\n",
    "\n",
    "split will be obtained every time the code is executed.\n",
    "\n",
    "The function returns four sets of data: X_train (training features), X_test (test features), y_train (training labels), and \n",
    "\n",
    "y_test (test labels).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "86896032",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "111f015d",
   "metadata": {},
   "source": [
    "StandardScaler() from scikit-learn is an object used for standardizing features by removing the mean and scaling to unit variance.\n",
    "\n",
    "scaler.fit_transform(X_train) fits the scaler on the training set (X_train) and then transforms it.\n",
    "\n",
    "This means the scaler calculates the mean and standard deviation from the training set and applies the transformation to \n",
    "\n",
    "standardize the features.\n",
    "\n",
    "scaler.transform(X_test) applies the same transformation to the test set (X_test) using the mean and standard deviation\n",
    "\n",
    "calculated from the training set. \n",
    "\n",
    "This ensures that the test set is scaled consistently with the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7d0ed1c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert numpy arrays to PyTorch tensors\n",
    "X_train = torch.from_numpy(X_train).float()\n",
    "X_test = torch.from_numpy(X_test).float()\n",
    "y_train = torch.from_numpy(y_train).long()\n",
    "y_test = torch.from_numpy(y_test).long()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b662050",
   "metadata": {},
   "source": [
    "torch.from_numpy() is a function in PyTorch that creates a tensor from a NumPy array.\n",
    "\n",
    ".float() is used to convert the tensor to the float data type. This is typically done for input features.\n",
    "\n",
    ".long() is used to convert the tensor to the long data type. This is typically done for target labels.\n",
    "\n",
    "By converting the NumPy arrays to PyTorch tensors, we enable seamless integration with PyTorch operations and take advantage of PyTorch's automatic differentiation capabilities during the training process. It allows us to perform computations on the GPU if available and utilize the vast ecosystem of PyTorch libraries and functionalities for deep learning tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1b85d9e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a custom dataset\n",
    "class IrisDataset(Dataset):\n",
    "    def __init__(self, features, labels):\n",
    "        self.features = features\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x = self.features[index]\n",
    "        y = self.labels[index]\n",
    "        return x, y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92f0ed22",
   "metadata": {},
   "source": [
    "The __init__ method is the initializer of the class. It takes two arguments: features and labels. These arguments represent the input features and target labels for the Iris dataset.\n",
    "In the __init__ method, the input features and labels are assigned to instance variables self.features and self.labels, respectively.\n",
    "The __getitem__ method is implemented to retrieve an item from the dataset given an index. It takes an index as input and returns the corresponding features (x) and labels (y) at that index.\n",
    "The __len__ method returns the total number of samples in the dataset, which is determined by the length of the features array.\n",
    "By creating this custom dataset class, we can leverage the functionality provided by PyTorch's DataLoader class to efficiently load and process the Iris dataset during training and testing. The custom dataset encapsulates the features and labels, allowing us to easily retrieve individual samples when iterating over the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1ef2abe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the neural network\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "093cfb14",
   "metadata": {},
   "source": [
    "The NeuralNetwork class is defined as a subclass of nn.Module, which is the base class for all neural network modules in PyTorch.\n",
    "\n",
    "The __init__ method is the initialization method of the class. It is called when an instance of the class is created. Here, it takes three arguments: input_size, hidden_size, and num_classes.\n",
    "\n",
    "Inside the __init__ method, the structure of the neural network is defined by instantiating different layers.\n",
    "\n",
    "self.fc1 = nn.Linear(input_size, hidden_size) defines the first fully connected layer with input_size input features and hidden_size output features. This layer performs a linear transformation on the input data.\n",
    "\n",
    "self.relu = nn.ReLU() defines the activation function to be applied element-wise after the first fully connected layer. ReLU (Rectified Linear Unit) is a popular choice for introducing non-linearity in neural networks.\n",
    "\n",
    "self.fc2 = nn.Linear(hidden_size, num_classes) defines the second fully connected layer with hidden_size input features and num_classes output features. This layer transforms the data into the desired output size.\n",
    "\n",
    "The forward method defines the forward pass computation of the neural network. It takes an input tensor x as an argument and defines the sequence of operations to be applied to the input.\n",
    "\n",
    "Inside the forward method, the input tensor x is passed through the layers of the neural network in the defined order.\n",
    "The output tensor out is returned at the end of the forward method.\n",
    "\n",
    "By defining the neural network model in this way, we can create an instance of the NeuralNetwork class, which represents our customized neural network architecture. This allows us to easily access and manipulate the model's parameters, perform forward pass computations, and train the model on data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6af40e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set hyperparameters\n",
    "input_size = X_train.shape[1]\n",
    "hidden_size = 64\n",
    "num_classes = len(iris.target_names)\n",
    "learning_rate = 0.001\n",
    "batch_size = 16\n",
    "num_epochs = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69e00b2e",
   "metadata": {},
   "source": [
    "In the provided code, hyperparameters for training the neural network on the Iris dataset are set. These hyperparameters determine the configuration and behavior of the neural network during the training process. Let's break down each hyperparameter:\n",
    "\n",
    "input_size represents the number of input features in the dataset. In this case, it is set to the number of columns in the training set (X_train.shape[1]), indicating the size of the input layer of the neural network.\n",
    "\n",
    "hidden_size represents the number of units/neurons in the hidden layer of the neural network. It is set to 64 in this example, but it can be adjusted based on the complexity of the problem.\n",
    "\n",
    "num_classes corresponds to the number of classes in the classification problem. It is determined by the length ofiris.target_names, which represents the different target labels in the Iris dataset.\n",
    "\n",
    "learning_rate determines the step size at each iteration during gradient descent optimization. It controls how much the weights are updated based on the calculated gradients\n",
    "\n",
    "batch_size defines the number of samples in each mini-batch during training. It indicates how many samples are processed before updating the weights and biases of the neural network\n",
    "\n",
    "num_epochs represents the number of times the entire training dataset is passed through the neural network during training. It defines the number of iterations over the entire dataset.\n",
    "\n",
    "By setting these hyperparameters, we can configure the network's architecture, control the learning process, and define the number of iterations required for training the neural network on the Iris dataset. These hyperparameters can be adjusted based on the specific requirements of the problem and the characteristics of the dataset.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "`\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b6380880",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the dataloaders\n",
    "train_dataset = IrisDataset(X_train, y_train)\n",
    "test_dataset = IrisDataset(X_test, y_test)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48b63983",
   "metadata": {},
   "source": [
    "train_dataset is an instance of the IrisDataset class, created using the training features (X_train) and labels (y_train).\n",
    "\n",
    "test_dataset is an instance of the IrisDataset class, created using the test features (X_test) and labels (y_test).\n",
    "\n",
    "train_dataloader is a DataLoader object that wraps the train_dataset and provides an efficient way to iterate over the\n",
    "\n",
    "training samples in mini-batches. It takes the train_dataset as input and specifies the batch_size for each mini-batch.\n",
    "\n",
    "Additionally, setting shuffle=True ensures that the samples are randomly shuffled at the beginning of each epoch during training, improving the learning process.\n",
    "\n",
    "test_dataloader is a DataLoader object that wraps the test_dataset and is used for evaluating the performance of the\n",
    "\n",
    "trained model on the test set. Similar to train_dataloader, it specifies the batch_size for processing the test samples.\n",
    "\n",
    "Setting shuffle=False ensures that the samples are processed in the order they appear in the test dataset, maintaining consistency for evaluation purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cbbdc087",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the neural network\n",
    "model = NeuralNetwork(input_size, hidden_size, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "559139f7",
   "metadata": {},
   "source": [
    "NeuralNetwork is a custom class that represents the architecture of the neural network. It takes three arguments:\n",
    "\n",
    "input_size, hidden_size, and num_classes.\n",
    "\n",
    "input_size is the number of input features in the dataset. It determines the size of the input layer of the neural network.\n",
    "\n",
    "hidden_size is the number of units/neurons in the hidden layer of the neural network. It specifies the size and complexity \n",
    "\n",
    "of the hidden layer.\n",
    "\n",
    "num_classes is the number of classes in the classification problem. It defines the size of the output layer of the neural\n",
    "\n",
    "network, which is typically equal to the number of classes.\n",
    "\n",
    "By initializing the neural network model using the NeuralNetwork class, we define the architecture and parameters of the\n",
    "\n",
    "model. The model can then be trained on the training dataset and used for making predictions on new data. The specific\n",
    "\n",
    "implementation details of the NeuralNetwork class are not provided in the code snippet but would include the definition of the layers, activation functions, and forward pass logic of the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "30403fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acaddb0f",
   "metadata": {},
   "source": [
    "criterion is an instance of the nn.CrossEntropyLoss() class from PyTorch. This loss function is commonly used for multi-\n",
    "\n",
    "class classification problems. It combines the softmax activation function and the negative log likelihood loss,\n",
    "\n",
    "making it suitable for training neural networks to classify multiple classes.\n",
    "\n",
    "optimizer is an instance of the optim.Adam() class from PyTorch. It is used to optimize the model's parameters during the\n",
    "\n",
    "training process. The Adam optimizer is a popular optimization algorithm that utilizes adaptive learning rates and \n",
    "\n",
    "momentum. It updates the model's parameters based on the gradients computed during backpropagation.\n",
    "\n",
    "The loss function (criterion) measures the discrepancy between the predicted class probabilities and the true class labels.\n",
    "\n",
    "The optimizer (optimizer) adjusts the model's parameters in the direction that minimizes the loss function. By using these\n",
    "\n",
    "components together, the neural network can be trained to minimize the loss and improve its predictive performance.\n",
    "\n",
    "It's important to note that the specific choice of the loss function and optimizer can vary depending on the problem and the characteristics of the dataset. The nn.CrossEntropyLoss() and optim.Adam() functions are commonly used defaults, but other options may be suitable in different scenarios.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "70c4accc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Step [5/8], Loss: 1.1945\n",
      "Epoch [2/100], Step [5/8], Loss: 1.1145\n",
      "Epoch [3/100], Step [5/8], Loss: 0.9424\n",
      "Epoch [4/100], Step [5/8], Loss: 0.8364\n",
      "Epoch [5/100], Step [5/8], Loss: 0.7992\n",
      "Epoch [6/100], Step [5/8], Loss: 0.7936\n",
      "Epoch [7/100], Step [5/8], Loss: 0.6044\n",
      "Epoch [8/100], Step [5/8], Loss: 0.6036\n",
      "Epoch [9/100], Step [5/8], Loss: 0.5390\n",
      "Epoch [10/100], Step [5/8], Loss: 0.5171\n",
      "Epoch [11/100], Step [5/8], Loss: 0.4455\n",
      "Epoch [12/100], Step [5/8], Loss: 0.5367\n",
      "Epoch [13/100], Step [5/8], Loss: 0.4614\n",
      "Epoch [14/100], Step [5/8], Loss: 0.5400\n",
      "Epoch [15/100], Step [5/8], Loss: 0.3863\n",
      "Epoch [16/100], Step [5/8], Loss: 0.4385\n",
      "Epoch [17/100], Step [5/8], Loss: 0.3612\n",
      "Epoch [18/100], Step [5/8], Loss: 0.3908\n",
      "Epoch [19/100], Step [5/8], Loss: 0.3229\n",
      "Epoch [20/100], Step [5/8], Loss: 0.3117\n",
      "Epoch [21/100], Step [5/8], Loss: 0.4300\n",
      "Epoch [22/100], Step [5/8], Loss: 0.1681\n",
      "Epoch [23/100], Step [5/8], Loss: 0.2430\n",
      "Epoch [24/100], Step [5/8], Loss: 0.2964\n",
      "Epoch [25/100], Step [5/8], Loss: 0.2276\n",
      "Epoch [26/100], Step [5/8], Loss: 0.2299\n",
      "Epoch [27/100], Step [5/8], Loss: 0.2495\n",
      "Epoch [28/100], Step [5/8], Loss: 0.2563\n",
      "Epoch [29/100], Step [5/8], Loss: 0.2931\n",
      "Epoch [30/100], Step [5/8], Loss: 0.1826\n",
      "Epoch [31/100], Step [5/8], Loss: 0.3288\n",
      "Epoch [32/100], Step [5/8], Loss: 0.3146\n",
      "Epoch [33/100], Step [5/8], Loss: 0.2623\n",
      "Epoch [34/100], Step [5/8], Loss: 0.3379\n",
      "Epoch [35/100], Step [5/8], Loss: 0.1978\n",
      "Epoch [36/100], Step [5/8], Loss: 0.1970\n",
      "Epoch [37/100], Step [5/8], Loss: 0.1813\n",
      "Epoch [38/100], Step [5/8], Loss: 0.1287\n",
      "Epoch [39/100], Step [5/8], Loss: 0.2101\n",
      "Epoch [40/100], Step [5/8], Loss: 0.2555\n",
      "Epoch [41/100], Step [5/8], Loss: 0.2751\n",
      "Epoch [42/100], Step [5/8], Loss: 0.1429\n",
      "Epoch [43/100], Step [5/8], Loss: 0.1655\n",
      "Epoch [44/100], Step [5/8], Loss: 0.1285\n",
      "Epoch [45/100], Step [5/8], Loss: 0.1414\n",
      "Epoch [46/100], Step [5/8], Loss: 0.1904\n",
      "Epoch [47/100], Step [5/8], Loss: 0.0870\n",
      "Epoch [48/100], Step [5/8], Loss: 0.1036\n",
      "Epoch [49/100], Step [5/8], Loss: 0.1123\n",
      "Epoch [50/100], Step [5/8], Loss: 0.1349\n",
      "Epoch [51/100], Step [5/8], Loss: 0.1377\n",
      "Epoch [52/100], Step [5/8], Loss: 0.1487\n",
      "Epoch [53/100], Step [5/8], Loss: 0.2162\n",
      "Epoch [54/100], Step [5/8], Loss: 0.1523\n",
      "Epoch [55/100], Step [5/8], Loss: 0.1811\n",
      "Epoch [56/100], Step [5/8], Loss: 0.0786\n",
      "Epoch [57/100], Step [5/8], Loss: 0.0963\n",
      "Epoch [58/100], Step [5/8], Loss: 0.1407\n",
      "Epoch [59/100], Step [5/8], Loss: 0.2429\n",
      "Epoch [60/100], Step [5/8], Loss: 0.1469\n",
      "Epoch [61/100], Step [5/8], Loss: 0.1878\n",
      "Epoch [62/100], Step [5/8], Loss: 0.2374\n",
      "Epoch [63/100], Step [5/8], Loss: 0.0641\n",
      "Epoch [64/100], Step [5/8], Loss: 0.0515\n",
      "Epoch [65/100], Step [5/8], Loss: 0.0744\n",
      "Epoch [66/100], Step [5/8], Loss: 0.1227\n",
      "Epoch [67/100], Step [5/8], Loss: 0.1292\n",
      "Epoch [68/100], Step [5/8], Loss: 0.0760\n",
      "Epoch [69/100], Step [5/8], Loss: 0.0558\n",
      "Epoch [70/100], Step [5/8], Loss: 0.1203\n",
      "Epoch [71/100], Step [5/8], Loss: 0.1022\n",
      "Epoch [72/100], Step [5/8], Loss: 0.0752\n",
      "Epoch [73/100], Step [5/8], Loss: 0.0514\n",
      "Epoch [74/100], Step [5/8], Loss: 0.0367\n",
      "Epoch [75/100], Step [5/8], Loss: 0.0358\n",
      "Epoch [76/100], Step [5/8], Loss: 0.1684\n",
      "Epoch [77/100], Step [5/8], Loss: 0.0540\n",
      "Epoch [78/100], Step [5/8], Loss: 0.0622\n",
      "Epoch [79/100], Step [5/8], Loss: 0.0654\n",
      "Epoch [80/100], Step [5/8], Loss: 0.0371\n",
      "Epoch [81/100], Step [5/8], Loss: 0.1313\n",
      "Epoch [82/100], Step [5/8], Loss: 0.0793\n",
      "Epoch [83/100], Step [5/8], Loss: 0.2078\n",
      "Epoch [84/100], Step [5/8], Loss: 0.0549\n",
      "Epoch [85/100], Step [5/8], Loss: 0.1420\n",
      "Epoch [86/100], Step [5/8], Loss: 0.1386\n",
      "Epoch [87/100], Step [5/8], Loss: 0.0837\n",
      "Epoch [88/100], Step [5/8], Loss: 0.0397\n",
      "Epoch [89/100], Step [5/8], Loss: 0.0721\n",
      "Epoch [90/100], Step [5/8], Loss: 0.1043\n",
      "Epoch [91/100], Step [5/8], Loss: 0.0535\n",
      "Epoch [92/100], Step [5/8], Loss: 0.0698\n",
      "Epoch [93/100], Step [5/8], Loss: 0.0282\n",
      "Epoch [94/100], Step [5/8], Loss: 0.0652\n",
      "Epoch [95/100], Step [5/8], Loss: 0.0707\n",
      "Epoch [96/100], Step [5/8], Loss: 0.0421\n",
      "Epoch [97/100], Step [5/8], Loss: 0.0976\n",
      "Epoch [98/100], Step [5/8], Loss: 0.0880\n",
      "Epoch [99/100], Step [5/8], Loss: 0.0135\n",
      "Epoch [100/100], Step [5/8], Loss: 0.1654\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (inputs, labels) in enumerate(train_dataloader):\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (i + 1) % 5 == 0:\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(train_dataloader)}], Loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fb67060",
   "metadata": {},
   "source": [
    "The outer loop iterates over the specified number of epochs (num_epochs), representing the number of times the entire \n",
    "\n",
    "training dataset is passed through the neural network.\n",
    "\n",
    "The inner loop iterates over the mini-batches of the training dataset provided by the train_dataloader.\n",
    "\n",
    "inputs and labels represent a mini-batch of input features and corresponding target labels.\n",
    "\n",
    "The forward pass is performed by passing the inputs through the model, which computes the predicted outputs.\n",
    "\n",
    "The loss is calculated using the specified loss function (criterion) by comparing the predicted outputs with the true labels.\n",
    "\n",
    "The optimizer's gradient buffers are cleared (optimizer.zero_grad()) to ensure that gradients from the previous iteration do not accumulate.\n",
    "\n",
    "The gradients are computed by calling loss.backward(), which calculates the gradients of the loss with respect to the model's parameters using backpropagation.\n",
    "\n",
    "The optimizer's step() function is called to update the model's parameters based on the computed gradients.\n",
    "\n",
    "The if condition checks if the current iteration is a multiple of 5. If true, it prints the current epoch, iteration, and loss value.\n",
    "\n",
    "This printing statement is optional and is commonly used to monitor the training progress and observe the decreasing loss value.\n",
    "\n",
    "By executing this training loop, the neural network iteratively learns from the training dataset, adjusting its parameters to minimize the loss. The process involves forwarding the inputs through the model, calculating the loss, backpropagating the gradients, and updating the model's parameters using the optimizer. This loop is repeated for the specified number of epochs, allowing the network to converge and improve its performance on the task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "25e6751d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 100.00%\n"
     ]
    }
   ],
   "source": [
    "# Evaluation\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for inputs, labels in test_dataloader:\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f\"Test Accuracy: {accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b2d6fed",
   "metadata": {},
   "source": [
    "model.eval() sets the model to evaluation mode. This is necessary to turn off certain operations like dropout or batch normalization, which behave differently during training and evaluation.\n",
    "\n",
    "with torch.no_grad(): is a context manager that disables gradient calculation. It is used during evaluation to save memory and speed up computations since gradients are not needed for evaluation.\n",
    "\n",
    "correct and total variables are initialized to keep track of the number of correctly classified samples and the total number of samples in the test dataset, respectively.\n",
    "\n",
    "The code then iterates over the test dataset using the test_dataloader.\n",
    "\n",
    "For each mini-batch of inputs and labels, the model performs a forward pass (outputs = model(inputs)).\n",
    "\n",
    "The predicted class labels are obtained by taking the maximum value along the second dimension (1) of the outputs tensor using torch.max(outputs.data, 1). The _ variable is used to store the maximum values, while predicted stores the corresponding predicted class labels.\n",
    "\n",
    "labels.size(0) gives the number of labels in the current mini-batch, which is added to the total count.\n",
    "(predicted == labels).sum().item() calculates the number of correctly classified samples in the mini-batch and adds it to the correct count.\n",
    "\n",
    "After iterating over all mini-batches in the test dataset, the accuracy is calculated by dividing the correct count by the total count and multiplying by 100.\n",
    "Finally, the test accuracy is printed.\n",
    "\n",
    "By evaluating the model on the test dataset, we can assess how well it generalizes to unseen data and obtain a measure of its performance in terms of accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf5e096",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
